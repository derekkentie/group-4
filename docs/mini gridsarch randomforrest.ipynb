{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "041d02cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle #to un-encode the dictionaries\n",
    "from pathlib import Path #used for looping through representation dictionaries\n",
    "from itertools import product #used for gridsearch on hyperparameters\n",
    "\n",
    "#data processing libraries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#model libraries\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30b94f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "proteindictspath = Path(r\"C:\\Users\\20243625\\OneDrive - TU Eindhoven\\Desktop\\group-4\\docs\\Sep's picklebestanden\\protein dicts to use in gridsearch\").glob('*')\n",
    "moldictspath = Path(r\"C:\\Users\\20243625\\OneDrive - TU Eindhoven\\Desktop\\group-4\\docs\\mol representatie picklebestanden\").glob('*')\n",
    "combinations = []\n",
    "for protpath in proteindictspath:\n",
    "    # print(protpath)\n",
    "    moldictspath = Path(r\"C:\\Users\\20243625\\OneDrive - TU Eindhoven\\Desktop\\group-4\\docs\\mol representatie picklebestanden\").glob('*')\n",
    "    for molpath in moldictspath:\n",
    "        if 'train' in molpath.stem:\n",
    "            combinations.append((protpath,molpath))\n",
    "            # print('1')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cd636606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "print(len(combinations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0146f47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(data_location):\n",
    "        data = pd.read_csv(rf\"{data_location}\")\n",
    "        return data\n",
    "traindata = data_loader(r\"C:\\Users\\20243625\\OneDrive - TU Eindhoven\\Desktop\\group-4\\data\\train.csv\")\n",
    "testdata = data_loader(r\"C:\\Users\\20243625\\OneDrive - TU Eindhoven\\Desktop\\group-4\\data\\test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31e7b4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Train score: 0.9243730871252673\n",
      "Test score: 0.45191905564364765\n",
      "abs_loss 2.8096194918495603\n",
      "2\n",
      "Train score: 0.9310305612545288\n",
      "Test score: 0.5023131799100911\n",
      "abs_loss 2.7230725966010914\n"
     ]
    }
   ],
   "source": [
    "#selecting the preferred dicitionaries\n",
    "iteration = 1\n",
    "for protdict,moltraindict in combinations[0:2]:\n",
    "    molecule_features_dict_train = pickle.load(moltraindict.open('rb'))\n",
    "    molecule_features_dict_test = pickle.load(open(str(moltraindict).replace('train','test'),'rb'))\n",
    "    protein_features_dict = pickle.load(protdict.open('rb'))\n",
    "    # print(\"dictionaries loaded\")\n",
    "    #loading the training set\n",
    "    train_df = pd.read_csv(r\"C:\\Users\\20243625\\OneDrive - TU Eindhoven\\Desktop\\group-4\\data\\train.csv\")\n",
    "    test_df = pd.read_csv(r\"C:\\Users\\20243625\\OneDrive - TU Eindhoven\\Desktop\\group-4\\data\\test.csv\")\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    #feature concatenation to combining each ligand-protein pair\n",
    "\n",
    "    for _, row in train_df.iterrows():\n",
    "        smiles = row[\"molecule_SMILES\"]\n",
    "        protein = row[\"UniProt_ID\"]\n",
    "        affinity_score = row[\"affinity_score\"]\n",
    "        #quick check if all elements are available\n",
    "        if smiles not in molecule_features_dict_train: \n",
    "            raise FileNotFoundError(\n",
    "                f\"The following SMILES exists in the trainingset but not in the molecule-features dictionary: {smiles}\"\n",
    "            )\n",
    "        if protein not in protein_features_dict: \n",
    "            raise FileNotFoundError(\n",
    "                f\"The following Uniprot_ID exists in the trainingset but not in the protein-features dictionary: {protein}\"\n",
    "            )\n",
    "\n",
    "        #feature concatenation\n",
    "        if isinstance(molecule_features_dict_train[smiles], np.ndarray):\n",
    "            molecule_features_dict_train[smiles] = molecule_features_dict_train[smiles].tolist()\n",
    "        if isinstance(protein_features_dict[protein], np.ndarray):\n",
    "            protein_features_dict[protein] = protein_features_dict[protein].tolist()\n",
    "        combined = molecule_features_dict_train[smiles] + protein_features_dict[protein]\n",
    "\n",
    "        #data seperation\n",
    "        X.append(combined)\n",
    "        y.append(affinity_score)\n",
    "\n",
    "    X = np.array(X, dtype=float)\n",
    "    y = np.array(y, dtype=float)\n",
    "\n",
    "    X_predict = []\n",
    "    for _, row in test_df.iterrows():\n",
    "        smiles = row[\"molecule_SMILES\"]\n",
    "        protein = row[\"UniProt_ID\"]\n",
    "        #quick check if all elements are available\n",
    "        if smiles not in molecule_features_dict_test: \n",
    "            raise FileNotFoundError(\n",
    "                f\"The following SMILES exists in the testset but not in the molecule-features dictionary: {smiles}\"\n",
    "            )\n",
    "        if protein not in protein_features_dict: \n",
    "            raise FileNotFoundError(\n",
    "                f\"The following Uniprot_ID exists in the testset but not in the protein-features dictionary: {protein}\"\n",
    "            )\n",
    "\n",
    "        #feature concatenation\n",
    "        if isinstance(molecule_features_dict_test[smiles], np.ndarray):\n",
    "            molecule_features_dict_test[smiles] = molecule_features_dict_test[smiles].tolist()\n",
    "        if isinstance(protein_features_dict[protein], np.ndarray):\n",
    "            protein_features_dict[protein] = protein_features_dict[protein].tolist()\n",
    "        combined = molecule_features_dict_test[smiles] + protein_features_dict[protein]\n",
    "\n",
    "        #data seperation\n",
    "        X_predict.append(combined)\n",
    "    X_predict = np.array(X_predict, dtype=float)\n",
    "    # print(\"feature concatenation complete\")\n",
    "\n",
    "    #splitting the data in training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        #ONLY CHANGE THE TEST_SIZE BY PREFERENCE\n",
    "        X, y, test_size=0.33, random_state=42 \n",
    "    )\n",
    "    # print(\"data splitting complete\")\n",
    "\n",
    "    #BELOW ARE OPTIONS FOR SCALING AND PCA, REMOVE DOCSTRINGS FOR\n",
    "    #THE PREFERRED OPTION(S)\n",
    "\n",
    "    #choose one of the following scaling option, or leave them out if preferred\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    X_predict = scaler.transform(X_predict)\n",
    "    print(\"standard scaling complete\")\n",
    "    \"\"\"\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    X = scaler.fit_transform(X)\n",
    "    #X_predict = scaler.transform(X_predict)\n",
    "    # print(\"minmax scaling complete\")\n",
    "\n",
    "    #apply PCA if preferred\n",
    "    r\"\"\"\n",
    "    ValueError: Input X contains NaN.\n",
    "    PCA does not accept missing values encoded as NaN natively. \n",
    "    For supervised learning, you might want to consider \n",
    "    sklearn.ensemble.HistGradientBoostingClassifier and Regressor \n",
    "    which accept missing values encoded as NaNs natively. \n",
    "    Alternatively, it is possible to preprocess the data, \n",
    "    for instance by using an imputer transformer in a pipeline \n",
    "    or drop samples with missing values. \n",
    "    See https://scikit-learn.org/stable/modules/impute.html \n",
    "    You can find a list of all estimators that handle NaN values \n",
    "    at the following page: \n",
    "    https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    pca = PCA(n_components=8)\n",
    "    X_train = pca.fit_transform(X_train)\n",
    "    X_test  = pca.transform(X_test)\n",
    "    print(\"PCA application complete\")\n",
    "    \"\"\"\n",
    "    #NOW APPLY YOUR PREFERRED MODEL TYPE\n",
    "    r\"\"\"\n",
    "    ValueError: Input X contains NaN.\n",
    "    MLPRegressor does not accept missing values encoded as NaN natively. \n",
    "    For supervised learning, you might want to consider \n",
    "    sklearn.ensemble.HistGradientBoostingClassifier and Regressor \n",
    "    which accept missing values encoded as NaNs natively. \n",
    "    Alternatively, it is possible to preprocess the data, \n",
    "    for instance by using an imputer transformer in a pipeline \n",
    "    or drop samples with missing values. \n",
    "    See https://scikit-learn.org/stable/modules/impute.html \n",
    "    You can find a list of all estimators that handle NaN values \n",
    "    at the following page: \n",
    "    https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    model = MLPRegressor(\n",
    "        hidden_layer_sizes=( 16, 8),\n",
    "        activation='logistic',\n",
    "        learning_rate='adaptive',\n",
    "        max_iter=400,\n",
    "        random_state=42\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    model = RandomForestRegressor(\n",
    "        max_features='sqrt',\n",
    "        max_depth=500\n",
    "    )\n",
    "\n",
    "\n",
    "    # model = HistGradientBoostingRegressor(\n",
    "    #     loss= \"absolute_error\",\n",
    "    #     learning_rate= 0.1,\n",
    "    #     max_iter= 100\n",
    "    # )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    print(iteration)\n",
    "    print(\"Train score:\", model.score(X_train, y_train))\n",
    "    print(\"Test score:\", model.score(X_test, y_test))\n",
    "    print('abs_loss',np.average(abs(model.predict(X_test)-y_test)))\n",
    "    iteration += 1\n",
    "\n",
    "#FOR MAKING THE ACTUAL PREDICTIONS\n",
    "\n",
    "# model.fit(X, y)\n",
    "# y_predict = model.predict(X_predict)\n",
    "\n",
    "# submission = pd.DataFrame({\n",
    "#     \"ID\": test_df[\"ID\"],\n",
    "#     \"affinity_score\": y_predict\n",
    "# })\n",
    "# submission.to_csv(\"data/submission2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e7b731dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "abs_loss 2.7147759103403097\n",
      "6\n",
      "abs_loss 2.693235040646847\n",
      "10\n",
      "abs_loss 2.716072929247253\n",
      "15\n",
      "abs_loss 2.711401051178075\n"
     ]
    }
   ],
   "source": [
    "# output = r\"\"\"C:\\Users\\20243625\\OneDrive - TU Eindhoven\\Desktop\\group-4\\docs\\mol representatie picklebestanden\\train_molecule_combined_representation.pkl\n",
    "# 1\n",
    "# Train score: 0.9198618309025929\n",
    "# Test score: 0.4132078638660188\n",
    "# 1\n",
    "# Train score: 0.9106905198968767\n",
    "# Test score: 0.36201019375805454\n",
    "# 1\n",
    "# Train score: 0.9154302764984878\n",
    "# Test score: 0.39535249872184675\n",
    "# 1\n",
    "# Train score: 0.9217580044408616\n",
    "# Test score: 0.4403328464414671\n",
    "# 1\n",
    "# Train score: 0.9144708846458509\n",
    "# Test score: 0.38929312470093436\n",
    "# 1\n",
    "# Train score: 0.916061305145672\n",
    "# Test score: 0.4175202727265849\n",
    "# 1\n",
    "# Train score: 0.9250683514242007\n",
    "# Test score: 0.4665612680171197\n",
    "# 1\n",
    "# Train score: 0.9198473856543024\n",
    "# Test score: 0.4308613345462996\n",
    "# 1\n",
    "# Train score: 0.9207919543543407\n",
    "# Test score: 0.4333794372279679\n",
    "# 1\n",
    "# Train score: 0.9160679349490032\n",
    "# Test score: 0.4060377352501878\n",
    "# 1\n",
    "# Train score: 0.9098172481334615\n",
    "# Test score: 0.3555330787816271\n",
    "# 1\n",
    "# Train score: 0.9125425638520753\n",
    "# Test score: 0.3872406418381015\n",
    "# 1\n",
    "# Train score: 0.9209937180786263\n",
    "# Test score: 0.4374619619907618\n",
    "# 1\n",
    "# Train score: 0.913969733032996\n",
    "# Test score: 0.38382895900473246\n",
    "# 1\n",
    "# Train score: 0.9180675592138209\n",
    "# Test score: 0.41368486171675833\n",
    "# 1\n",
    "# Train score: 0.9255721339825949\n",
    "# Test score: 0.4676036406847044\n",
    "# 1\n",
    "# Train score: 0.9207632928689484\n",
    "# Test score: 0.42859873959420247\n",
    "# 1\n",
    "# Train score: 0.9185952247691547\n",
    "# Test score: 0.4347679750982181\n",
    "# 1\n",
    "# Train score: 0.9187214559759393\n",
    "# Test score: 0.4246162106944663\n",
    "# 1\n",
    "# Train score: 0.9105303881704226\n",
    "# Test score: 0.35639243847420454\n",
    "# 1\n",
    "# Train score: 0.9139819061436268\n",
    "# Test score: 0.3911542199450856\n",
    "# 1\n",
    "# Train score: 0.9223797794144107\n",
    "# Test score: 0.4397697100044584\n",
    "# 1\n",
    "# Train score: 0.9148718261798853\n",
    "# Test score: 0.3857563952715357\n",
    "# 1\n",
    "# Train score: 0.9157622810587089\n",
    "# Test score: 0.41379466995774206\n",
    "# 1\n",
    "# Train score: 0.926512018438649\n",
    "# Test score: 0.4724489172044446\n",
    "# 1\n",
    "# Train score: 0.9207637288881297\n",
    "# Test score: 0.4290221167193343\n",
    "# 1\n",
    "# Train score: 0.9203528751850473\n",
    "# Test score: 0.433935930381563\n",
    "# 1\n",
    "# Train score: 0.9170832375139956\n",
    "# Test score: 0.4142223967784584\n",
    "# 1\n",
    "# Train score: 0.9110514358787315\n",
    "# Test score: 0.3598677604133249\n",
    "# 1\n",
    "# Train score: 0.9142637208655607\n",
    "# Test score: 0.3961998937286748\n",
    "# 1\n",
    "# Train score: 0.9222773829548987\n",
    "# Test score: 0.44225718619046717\n",
    "# 1\n",
    "# Train score: 0.9156944269539136\n",
    "# Test score: 0.39356215493202595\n",
    "# 1\n",
    "# Train score: 0.9174315538682665\n",
    "# Test score: 0.414358345589078\n",
    "# 1\n",
    "# Train score: 0.9266605621113979\n",
    "# Test score: 0.47176276787646687\n",
    "# 1\n",
    "# Train score: 0.921003536178896\n",
    "# Test score: 0.42859143514024356\n",
    "# 1\n",
    "# Train score: 0.9206659385080908\n",
    "# Test score: 0.44184176288002397\n",
    "# 1\n",
    "# Train score: 0.916425021746666\n",
    "# Test score: 0.39922086416099656\n",
    "# 1\n",
    "# Train score: 0.9091399607592185\n",
    "# Test score: 0.35485144633389387\n",
    "# 1\n",
    "# Train score: 0.9115055585777478\n",
    "# Test score: 0.3806216695039252\n",
    "# 1\n",
    "# Train score: 0.920523085977137\n",
    "# Test score: 0.4300408628852409\n",
    "# 1\n",
    "# Train score: 0.9133219471646856\n",
    "# Test score: 0.3763656184994568\n",
    "# 1\n",
    "# Train score: 0.9151229854636909\n",
    "# Test score: 0.39676229678832753\n",
    "# 1\n",
    "# Train score: 0.9242396977946533\n",
    "# Test score: 0.4560898976125527\n",
    "# 1\n",
    "# Train score: 0.9194591526571937\n",
    "# Test score: 0.4078578132759265\n",
    "# 1\n",
    "# Train score: 0.9190343085311231\n",
    "# Test score: 0.42505981364577494\n",
    "# 1\n",
    "# Train score: 0.9153051057587928\n",
    "# Test score: 0.4012128089448719\n",
    "# 1\n",
    "# Train score: 0.9089037121868182\n",
    "# Test score: 0.35214052568063947\n",
    "# 1\n",
    "# Train score: 0.9120395417959742\n",
    "# Test score: 0.3810440247544239\n",
    "# 1\n",
    "# Train score: 0.9202395703313662\n",
    "# Test score: 0.43217193920614616\n",
    "# 1\n",
    "# Train score: 0.9121591098141268\n",
    "# Test score: 0.3696351776242641\n",
    "# 1\n",
    "# Train score: 0.9158154278356758\n",
    "# Test score: 0.406662500179213\n",
    "# 1\n",
    "# Train score: 0.9232536486275712\n",
    "# Test score: 0.4522030778989492\n",
    "# 1\n",
    "# Train score: 0.9173823353142686\n",
    "# Test score: 0.4125953431073084\n",
    "# 1\n",
    "# Train score: 0.9190869220140677\n",
    "# Test score: 0.4301017168060264\n",
    "# \"\"\"\n",
    "output_randomforrest = r\"\"\"1\n",
    "Train score: 0.9198618309025929\n",
    "Test score: 0.4132078638660188\n",
    "2\n",
    "Train score: 0.9106905198968767\n",
    "Test score: 0.36201019375805454\n",
    "3\n",
    "Train score: 0.9154302764984878\n",
    "Test score: 0.39535249872184675\n",
    "4\n",
    "Train score: 0.9217580044408616\n",
    "Test score: 0.4403328464414671\n",
    "5\n",
    "Train score: 0.9144708846458509\n",
    "Test score: 0.38929312470093436\n",
    "6\n",
    "Train score: 0.916061305145672\n",
    "Test score: 0.4175202727265849\n",
    "7\n",
    "Train score: 0.9250683514242007\n",
    "Test score: 0.4665612680171197\n",
    "8\n",
    "Train score: 0.9198473856543024\n",
    "Test score: 0.4308613345462996\n",
    "9\n",
    "Train score: 0.9207919543543407\n",
    "Test score: 0.4333794372279679\n",
    "10\n",
    "Train score: 0.9160679349490032\n",
    "Test score: 0.4060377352501878\n",
    "11\n",
    "Train score: 0.9098172481334615\n",
    "Test score: 0.3555330787816271\n",
    "12\n",
    "Train score: 0.9125425638520753\n",
    "Test score: 0.3872406418381015\n",
    "13\n",
    "Train score: 0.9209937180786263\n",
    "Test score: 0.4374619619907618\n",
    "14\n",
    "Train score: 0.913969733032996\n",
    "Test score: 0.38382895900473246\n",
    "15\n",
    "Train score: 0.9180675592138209\n",
    "Test score: 0.41368486171675833\n",
    "16\n",
    "Train score: 0.9255721339825949\n",
    "Test score: 0.4676036406847044\n",
    "17\n",
    "Train score: 0.9207632928689484\n",
    "Test score: 0.42859873959420247\n",
    "18\n",
    "Train score: 0.9185952247691547\n",
    "Test score: 0.4347679750982181\n",
    "19\n",
    "Train score: 0.9187214559759393\n",
    "Test score: 0.4246162106944663\n",
    "20\n",
    "Train score: 0.9105303881704226\n",
    "Test score: 0.35639243847420454\n",
    "21\n",
    "Train score: 0.9139819061436268\n",
    "Test score: 0.3911542199450856\n",
    "22\n",
    "Train score: 0.9223797794144107\n",
    "Test score: 0.4397697100044584\n",
    "23\n",
    "Train score: 0.9148718261798853\n",
    "Test score: 0.3857563952715357\n",
    "24\n",
    "Train score: 0.9157622810587089\n",
    "Test score: 0.41379466995774206\n",
    "25\n",
    "Train score: 0.926512018438649\n",
    "Test score: 0.4724489172044446\n",
    "26\n",
    "Train score: 0.9207637288881297\n",
    "Test score: 0.4290221167193343\n",
    "27\n",
    "Train score: 0.9203528751850473\n",
    "Test score: 0.433935930381563\n",
    "28\n",
    "Train score: 0.9170832375139956\n",
    "Test score: 0.4142223967784584\n",
    "29\n",
    "Train score: 0.9110514358787315\n",
    "Test score: 0.3598677604133249\n",
    "30\n",
    "Train score: 0.9142637208655607\n",
    "Test score: 0.3961998937286748\n",
    "31\n",
    "Train score: 0.9222773829548987\n",
    "Test score: 0.44225718619046717\n",
    "32\n",
    "Train score: 0.9156944269539136\n",
    "Test score: 0.39356215493202595\n",
    "33\n",
    "Train score: 0.9174315538682665\n",
    "Test score: 0.414358345589078\n",
    "34\n",
    "Train score: 0.9266605621113979\n",
    "Test score: 0.47176276787646687\n",
    "35\n",
    "Train score: 0.921003536178896\n",
    "Test score: 0.42859143514024356\n",
    "36\n",
    "Train score: 0.9206659385080908\n",
    "Test score: 0.44184176288002397\n",
    "37\n",
    "Train score: 0.916425021746666\n",
    "Test score: 0.39922086416099656\n",
    "38\n",
    "Train score: 0.9091399607592185\n",
    "Test score: 0.35485144633389387\n",
    "39\n",
    "Train score: 0.9115055585777478\n",
    "Test score: 0.3806216695039252\n",
    "40\n",
    "Train score: 0.920523085977137\n",
    "Test score: 0.4300408628852409\n",
    "41\n",
    "Train score: 0.9133219471646856\n",
    "Test score: 0.3763656184994568\n",
    "42\n",
    "Train score: 0.9151229854636909\n",
    "Test score: 0.39676229678832753\n",
    "43\n",
    "Train score: 0.9242396977946533\n",
    "Test score: 0.4560898976125527\n",
    "44\n",
    "Train score: 0.9194591526571937\n",
    "Test score: 0.4078578132759265\n",
    "45\n",
    "Train score: 0.9190343085311231\n",
    "Test score: 0.42505981364577494\n",
    "46\n",
    "Train score: 0.9153051057587928\n",
    "Test score: 0.4012128089448719\n",
    "47\n",
    "Train score: 0.9089037121868182\n",
    "Test score: 0.35214052568063947\n",
    "48\n",
    "Train score: 0.9120395417959742\n",
    "Test score: 0.3810440247544239\n",
    "49\n",
    "Train score: 0.9202395703313662\n",
    "Test score: 0.43217193920614616\n",
    "50\n",
    "Train score: 0.9121591098141268\n",
    "Test score: 0.3696351776242641\n",
    "51\n",
    "Train score: 0.9158154278356758\n",
    "Test score: 0.406662500179213\n",
    "52\n",
    "Train score: 0.9232536486275712\n",
    "Test score: 0.4522030778989492\n",
    "53\n",
    "Train score: 0.9173823353142686\n",
    "Test score: 0.4125953431073084\n",
    "54\n",
    "Train score: 0.9190869220140677\n",
    "Test score: 0.4301017168060264\"\"\"\n",
    "output_randomforrest2 = r\"\"\"1\n",
    "Train score: 0.9189687945638008\n",
    "Test score: 0.41469148611760565\n",
    "abs_loss 2.9011179845289137\n",
    "2\n",
    "Train score: 0.9088404357359092\n",
    "Test score: 0.3605281537805022\n",
    "abs_loss 3.0432322173215116\n",
    "3\n",
    "Train score: 0.9124694854838481\n",
    "Test score: 0.3944772527638962\n",
    "abs_loss 2.9374720896499626\n",
    "4\n",
    "Train score: 0.921558992840867\n",
    "Test score: 0.4382743095292425\n",
    "abs_loss 2.840040453257743\n",
    "5\n",
    "Train score: 0.9156341479067052\n",
    "Test score: 0.38497534249949805\n",
    "abs_loss 2.9877318653438314\n",
    "6\n",
    "Train score: 0.9168974684844132\n",
    "Test score: 0.4158640885519338\n",
    "abs_loss 2.886466770471716\n",
    "7\n",
    "Train score: 0.9254034000968875\n",
    "Test score: 0.4733139346289962\n",
    "abs_loss 2.7641662251913197\n",
    "8\n",
    "Train score: 0.9202767495583877\n",
    "Test score: 0.4199001223040578\n",
    "abs_loss 2.9041405328335124\n",
    "9\n",
    "Train score: 0.920651919165978\n",
    "Test score: 0.4342819014985675\n",
    "abs_loss 2.8502637899459\n",
    "10\n",
    "Train score: 0.9182462634277602\n",
    "Test score: 0.41440356910878773\n",
    "abs_loss 2.8952552225852557\n",
    "11\n",
    "Train score: 0.9096783329155272\n",
    "Test score: 0.35557601300645614\n",
    "abs_loss 3.0542966352278174\n",
    "12\n",
    "Train score: 0.913915054400281\n",
    "Test score: 0.38755663154605313\n",
    "abs_loss 2.956541770714819\n",
    "13\n",
    "Train score: 0.9224689468012227\n",
    "Test score: 0.4357489378519662\n",
    "abs_loss 2.854505178583582\n",
    "14\n",
    "Train score: 0.9150521587199245\n",
    "Test score: 0.38832996903752104\n",
    "abs_loss 2.9792626233748543\n",
    "15\n",
    "Train score: 0.9168116087448686\n",
    "Test score: 0.41094518685173886\n",
    "abs_loss 2.9009215829419364\n",
    "16\n",
    "Train score: 0.9257621791531205\n",
    "Test score: 0.4660272052513804\n",
    "abs_loss 2.7923912653033423\n",
    "17\n",
    "Train score: 0.9194630022563942\n",
    "Test score: 0.4258989575329335\n",
    "abs_loss 2.89653196774183\n",
    "18\n",
    "Train score: 0.9210379882149394\n",
    "Test score: 0.44200423773409137\n",
    "abs_loss 2.8216449253835334\n",
    "19\n",
    "Train score: 0.9177612556615404\n",
    "Test score: 0.4205659003907085\n",
    "abs_loss 2.8848561287928134\n",
    "20\n",
    "Train score: 0.9108690224205813\n",
    "Test score: 0.35624222894680013\n",
    "abs_loss 3.0619975639130677\n",
    "21\n",
    "Train score: 0.9143156186649758\n",
    "Test score: 0.3958791854523428\n",
    "abs_loss 2.9379964813248134\n",
    "22\n",
    "Train score: 0.9215824204772729\n",
    "Test score: 0.44201850543754695\n",
    "abs_loss 2.8459766487256526\n",
    "23\n",
    "Train score: 0.9146991697157911\n",
    "Test score: 0.3850272155123762\n",
    "abs_loss 2.98470296083446\n",
    "24\n",
    "Train score: 0.9156782414769025\n",
    "Test score: 0.41652312100588795\n",
    "abs_loss 2.8777024278712893\n",
    "25\n",
    "Train score: 0.9253493383137172\n",
    "Test score: 0.4672893214288679\n",
    "abs_loss 2.793780330604094\n",
    "26\n",
    "Train score: 0.9197698760945141\n",
    "Test score: 0.42468116611705087\n",
    "abs_loss 2.8960539426544676\n",
    "27\n",
    "Train score: 0.9204948479463643\n",
    "Test score: 0.4367254087810247\n",
    "abs_loss 2.8459815188113207\n",
    "28\n",
    "Train score: 0.9151682297979609\n",
    "Test score: 0.41488913996661525\n",
    "abs_loss 2.898720864896227\n",
    "29\n",
    "Train score: 0.909856600331276\n",
    "Test score: 0.3576499293494064\n",
    "abs_loss 3.0546849166642356\n",
    "30\n",
    "Train score: 0.9128694653344764\n",
    "Test score: 0.3928223883849141\n",
    "abs_loss 2.9427112545419756\n",
    "31\n",
    "Train score: 0.9219194166608577\n",
    "Test score: 0.4440344529839412\n",
    "abs_loss 2.837857998580276\n",
    "32\n",
    "Train score: 0.9157766275603879\n",
    "Test score: 0.3895406779221431\n",
    "abs_loss 2.9815172775163474\n",
    "33\n",
    "Train score: 0.9181265455362752\n",
    "Test score: 0.41151797417022984\n",
    "abs_loss 2.895700526318347\n",
    "34\n",
    "Train score: 0.9250993226343344\n",
    "Test score: 0.4711567420159749\n",
    "abs_loss 2.76861248626467\n",
    "35\n",
    "Train score: 0.9206486035597896\n",
    "Test score: 0.4280451274705117\n",
    "abs_loss 2.890837902943492\n",
    "36\n",
    "Train score: 0.9202880162945606\n",
    "Test score: 0.4407214671045959\n",
    "abs_loss 2.831733499129689\n",
    "37\n",
    "Train score: 0.9157097767571237\n",
    "Test score: 0.39918860533079015\n",
    "abs_loss 2.9396247929666712\n",
    "38\n",
    "Train score: 0.9094625127252879\n",
    "Test score: 0.3532603566333773\n",
    "abs_loss 3.055248015600445\n",
    "39\n",
    "Train score: 0.9121613875251504\n",
    "Test score: 0.3814650981511887\n",
    "abs_loss 2.9768723265945134\n",
    "40\n",
    "Train score: 0.9200384541950395\n",
    "Test score: 0.43192793646962757\n",
    "abs_loss 2.8636788364110175\n",
    "41\n",
    "Train score: 0.9128780899709011\n",
    "Test score: 0.3726207929801332\n",
    "abs_loss 3.0137590399016267\n",
    "42\n",
    "Train score: 0.9159636718837045\n",
    "Test score: 0.40714482970669763\n",
    "abs_loss 2.901415414118134\n",
    "43\n",
    "Train score: 0.923719058372384\n",
    "Test score: 0.4548410653470162\n",
    "abs_loss 2.8012705339517865\n",
    "44\n",
    "Train score: 0.9187464191079948\n",
    "Test score: 0.4097356114898375\n",
    "abs_loss 2.9319799352373157\n",
    "45\n",
    "Train score: 0.9189503722598085\n",
    "Test score: 0.4269817951488821\n",
    "abs_loss 2.853327275202504\n",
    "46\n",
    "Train score: 0.9163489288025973\n",
    "Test score: 0.3981944129872611\n",
    "abs_loss 2.9429854000343587\n",
    "47\n",
    "Train score: 0.9100951720585463\n",
    "Test score: 0.3545887741466107\n",
    "abs_loss 3.0442223227048624\n",
    "48\n",
    "Train score: 0.9121625425262131\n",
    "Test score: 0.3759527134005065\n",
    "abs_loss 2.976727704036484\n",
    "49\n",
    "Train score: 0.9201946716170712\n",
    "Test score: 0.43030612129153245\n",
    "abs_loss 2.863075322840999\n",
    "50\n",
    "Train score: 0.9124023601127401\n",
    "Test score: 0.3740129457615927\n",
    "abs_loss 3.0051276192745147\n",
    "51\n",
    "Train score: 0.91511739246267\n",
    "Test score: 0.4083939329812317\n",
    "abs_loss 2.913953349475382\n",
    "52\n",
    "Train score: 0.9234573471306683\n",
    "Test score: 0.45633990101813304\n",
    "abs_loss 2.8032973795099676\n",
    "53\n",
    "Train score: 0.9188538366353607\n",
    "Test score: 0.41511866297103317\n",
    "abs_loss 2.9170543302345773\n",
    "54\n",
    "Train score: 0.9188547054039172\n",
    "Test score: 0.425226576191564\n",
    "abs_loss 2.8567111297490193\"\"\"\n",
    "output_randomforrest3 = r\"\"\"1\n",
    "Train score: 0.9217068847945763\n",
    "Test score: 0.4506702899441173\n",
    "abs_loss 2.827148049826422\n",
    "2\n",
    "Train score: 0.930775540316791\n",
    "Test score: 0.5012352711334682\n",
    "abs_loss 2.7248949389973105\n",
    "3\n",
    "Train score: 0.9291194985768726\n",
    "Test score: 0.4966607662239446\n",
    "abs_loss 2.7147759103403097\n",
    "4\n",
    "Train score: 0.9238011588736426\n",
    "Test score: 0.46970470974645484\n",
    "abs_loss 2.7797186399350413\n",
    "5\n",
    "Train score: 0.9239566228399789\n",
    "Test score: 0.45804589744209845\n",
    "abs_loss 2.796727010949365\n",
    "6\n",
    "Train score: 0.9312931511402109\n",
    "Test score: 0.5078330272793058\n",
    "abs_loss 2.693235040646847\n",
    "7\n",
    "Train score: 0.929170989730366\n",
    "Test score: 0.486204858520877\n",
    "abs_loss 2.7358298897770243\n",
    "8\n",
    "Train score: 0.9239222196679715\n",
    "Test score: 0.45752888122360846\n",
    "abs_loss 2.7944785396630896\n",
    "9\n",
    "Train score: 0.9234728226710056\n",
    "Test score: 0.45579458552530483\n",
    "abs_loss 2.807490921474435\n",
    "10\n",
    "Train score: 0.9292097114689187\n",
    "Test score: 0.4992909299150754\n",
    "abs_loss 2.716072929247253\n",
    "11\n",
    "Train score: 0.9273519408823676\n",
    "Test score: 0.48677592717214\n",
    "abs_loss 2.740116983105949\n",
    "12\n",
    "Train score: 0.9248451274435582\n",
    "Test score: 0.46085267115875017\n",
    "abs_loss 2.7880522529990803\n",
    "13\n",
    "Train score: 0.9244178933234075\n",
    "Test score: 0.45930321700626575\n",
    "abs_loss 2.794259610774154\n",
    "14\n",
    "Train score: 0.9299881314530374\n",
    "Test score: 0.4996598570994316\n",
    "abs_loss 2.734000891297228\n",
    "15\n",
    "Train score: 0.9271544612919621\n",
    "Test score: 0.49820103994765663\n",
    "abs_loss 2.711401051178075\n",
    "16\n",
    "Train score: 0.9252457113139757\n",
    "Test score: 0.4675855356780697\n",
    "abs_loss 2.7822326368427692\n",
    "17\n",
    "Train score: 0.9218805891908307\n",
    "Test score: 0.44336010386713376\n",
    "abs_loss 2.833484976953818\n",
    "18\n",
    "Train score: 0.9267531997617291\n",
    "Test score: 0.48419446781920894\n",
    "abs_loss 2.740511121321441\n",
    "19\n",
    "Train score: 0.9257804774985677\n",
    "Test score: 0.4660239953152897\n",
    "abs_loss 2.775688304888788\n",
    "20\n",
    "Train score: 0.922703727347878\n",
    "Test score: 0.45119680154005115\n",
    "abs_loss 2.821271820696828\n",
    "21\n",
    "Train score: 0.9218774543443757\n",
    "Test score: 0.44586167771027096\n",
    "abs_loss 2.8285762279951103\n",
    "22\n",
    "Train score: 0.927374771513009\n",
    "Test score: 0.4804669049147319\n",
    "abs_loss 2.7453533193305883\n",
    "23\n",
    "Train score: 0.9271124172492266\n",
    "Test score: 0.47440018033876385\n",
    "abs_loss 2.753726318348452\n",
    "24\n",
    "Train score: 0.9218093188064336\n",
    "Test score: 0.4517709373794567\n",
    "abs_loss 2.8162865843519342\"\"\"\n",
    "# print(float(output_randomforrest2.split(\"\\n\")[33*4+3].split(' ')[1] ))\n",
    "for i in range(len(output_randomforrest3.split('\\n'))):\n",
    "    if i % 4 == 3 and (float((output_randomforrest3.split(\"\\n\")[i]).split(' ')[1]) < 2.72): #\n",
    "        print(i//4 + 1)\n",
    "        print(output_randomforrest3.split(\"\\n\")[i])\n",
    "        # print(' '.split(output.split(\"\\n\")[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ea483784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 (WindowsPath(\"C:/Users/20243625/OneDrive - TU Eindhoven/Desktop/group-4/docs/Sep's picklebestanden/protein dicts to use in gridsearch/dict ID to BLOSUM62 vector in 3 pieces\"), WindowsPath('C:/Users/20243625/OneDrive - TU Eindhoven/Desktop/group-4/docs/mol representatie picklebestanden/train_molecule_combined_representation.pkl'))\n",
      "6 (WindowsPath(\"C:/Users/20243625/OneDrive - TU Eindhoven/Desktop/group-4/docs/Sep's picklebestanden/protein dicts to use in gridsearch/dict ID to feature vector 2 in 2 pieces\"), WindowsPath('C:/Users/20243625/OneDrive - TU Eindhoven/Desktop/group-4/docs/mol representatie picklebestanden/train_molecule_combined_representation.pkl'))\n",
      "10 (WindowsPath(\"C:/Users/20243625/OneDrive - TU Eindhoven/Desktop/group-4/docs/Sep's picklebestanden/protein dicts to use in gridsearch/dict ID to feature vector in one-hot in 2 pieces\"), WindowsPath('C:/Users/20243625/OneDrive - TU Eindhoven/Desktop/group-4/docs/mol representatie picklebestanden/train_molecule_combined_representation.pkl'))\n",
      "15 (WindowsPath(\"C:/Users/20243625/OneDrive - TU Eindhoven/Desktop/group-4/docs/Sep's picklebestanden/protein dicts to use in gridsearch/dict ID to PAM250 vector in 3 pieces\"), WindowsPath('C:/Users/20243625/OneDrive - TU Eindhoven/Desktop/group-4/docs/mol representatie picklebestanden/train_molecule_combined_representation.pkl'))\n"
     ]
    }
   ],
   "source": [
    "for i in [2,5,9,14]:\n",
    "    print(i+1,combinations[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0ec5f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionaries loaded\n",
      "length X_predict 34626\n",
      "feature concatenation complete\n",
      "data splitting complete\n",
      "len X 14839\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 854 features, but MinMaxScaler is expecting 853 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 101\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlen X\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;28mlen\u001b[39m(X))\n\u001b[0;32m    100\u001b[0m X \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(X)\n\u001b[1;32m--> 101\u001b[0m X_predict \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(X_predict)\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminmax scaling complete\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m#apply PCA if preferred\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\20243625\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 313\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    315\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    316\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    318\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    319\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\20243625\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:534\u001b[0m, in \u001b[0;36mMinMaxScaler.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    530\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    532\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[1;32m--> 534\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m    535\u001b[0m     X,\n\u001b[0;32m    536\u001b[0m     copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy,\n\u001b[0;32m    537\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m_array_api\u001b[38;5;241m.\u001b[39msupported_float_dtypes(xp),\n\u001b[0;32m    538\u001b[0m     force_writeable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    539\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    540\u001b[0m     reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    541\u001b[0m )\n\u001b[0;32m    543\u001b[0m X \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_\n\u001b[0;32m    544\u001b[0m X \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_\n",
      "File \u001b[1;32mc:\\Users\\20243625\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:654\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 654\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_n_features(X, reset\u001b[38;5;241m=\u001b[39mreset)\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\20243625\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:443\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 443\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    444\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    445\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    446\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 854 features, but MinMaxScaler is expecting 853 features as input."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor,RandomForestRegressor\n",
    "\n",
    "#selecting the preferred dicitionaries\n",
    "molecule_features_dict_train = pickle.load(open(r\"C:\\Users\\20243625\\OneDrive - TU Eindhoven\\Desktop\\group-4\\docs\\mol representatie picklebestanden\\train_molecule_combined_representation.pkl\",'rb'))\n",
    "molecule_features_dict_test = pickle.load(open(r\"C:\\Users\\20243625\\OneDrive - TU Eindhoven\\Desktop\\group-4\\docs\\mol representatie picklebestanden\\test_molecule_combined_representation.pkl\",'rb'))\n",
    "protein_features_dict = pickle.load(open(r\"C:\\Users\\20243625\\OneDrive - TU Eindhoven\\Desktop\\group-4\\docs\\Sep's picklebestanden\\protein dicts to use in gridsearch\\dict ID to feature vector 2 in 2 pieces\", 'rb'))\n",
    "print(\"dictionaries loaded\")\n",
    "#loading the training set\n",
    "train_df = pd.read_csv(r\"C:\\Users\\20243625\\OneDrive - TU Eindhoven\\Desktop\\group-4\\data\\train.csv\")\n",
    "test_df = pd.read_csv(r\"C:\\Users\\20243625\\OneDrive - TU Eindhoven\\Desktop\\group-4\\data\\test.csv\")\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "#feature concatenation to combining each ligand-protein pair\n",
    "\n",
    "for _, row in train_df.iterrows():\n",
    "    smiles = row[\"molecule_SMILES\"]\n",
    "    protein = row[\"UniProt_ID\"]\n",
    "    affinity_score = row[\"affinity_score\"]\n",
    "    #quick check if all elements are available\n",
    "    if smiles not in molecule_features_dict_train: \n",
    "        raise FileNotFoundError(\n",
    "            f\"The following SMILES exists in the trainingset but not in the molecule-features dictionary: {smiles}\"\n",
    "        )\n",
    "    if protein not in protein_features_dict: \n",
    "        raise FileNotFoundError(\n",
    "            f\"The following Uniprot_ID exists in the trainingset but not in the protein-features dictionary: {protein}\"\n",
    "        )\n",
    "\n",
    "    #feature concatenation\n",
    "    if isinstance(molecule_features_dict_train[smiles], np.ndarray):\n",
    "        molecule_features_dict_train[smiles] = molecule_features_dict_train[smiles].tolist()\n",
    "    if isinstance(protein_features_dict[protein], np.ndarray):\n",
    "        protein_features_dict[protein] = protein_features_dict[protein].tolist()\n",
    "    combined = molecule_features_dict_train[smiles] + protein_features_dict[protein]\n",
    "\n",
    "    #data seperation\n",
    "    X.append(combined)\n",
    "    y.append(affinity_score)\n",
    "\n",
    "X = np.array(X, dtype=float)\n",
    "y = np.array(y, dtype=float)\n",
    "\n",
    "X_predict = []\n",
    "for _, row in test_df.iterrows():\n",
    "    smiles = row[\"molecule_SMILES\"]\n",
    "    protein = row[\"UniProt_ID\"]\n",
    "    #quick check if all elements are available\n",
    "    if smiles not in molecule_features_dict_test: \n",
    "        raise FileNotFoundError(\n",
    "            f\"The following SMILES exists in the testset but not in the molecule-features dictionary: {smiles}\"\n",
    "        )\n",
    "    if protein not in protein_features_dict: \n",
    "        raise FileNotFoundError(\n",
    "            f\"The following Uniprot_ID exists in the testset but not in the protein-features dictionary: {protein}\"\n",
    "        )\n",
    "\n",
    "    #feature concatenation\n",
    "    if isinstance(molecule_features_dict_test[smiles], np.ndarray):\n",
    "        molecule_features_dict_test[smiles] = molecule_features_dict_test[smiles].tolist()\n",
    "    if isinstance(protein_features_dict[protein], np.ndarray):\n",
    "        protein_features_dict[protein] = protein_features_dict[protein].tolist()\n",
    "    combined = molecule_features_dict_test[smiles] + protein_features_dict[protein]\n",
    "\n",
    "    #data seperation\n",
    "    X_predict.append(combined)\n",
    "X_predict = np.array(X_predict, dtype=float)\n",
    "print('length X_predict',len(X_predict))\n",
    "print(\"feature concatenation complete\")\n",
    "\n",
    "#splitting the data in training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    #ONLY CHANGE THE TEST_SIZE BY PREFERENCE\n",
    "    X, y, test_size=0.33, random_state=42 \n",
    ")\n",
    "print(\"data splitting complete\")\n",
    "\n",
    "#BELOW ARE OPTIONS FOR SCALING AND PCA, REMOVE DOCSTRINGS FOR\n",
    "#THE PREFERRED OPTION(S)\n",
    "\n",
    "#choose one of the following scaling option, or leave them out if preferred\n",
    "\"\"\"\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_predict = scaler.transform(X_predict)\n",
    "print(\"standard scaling complete\")\n",
    "\"\"\"\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "print('len X',len(X))\n",
    "X = scaler.fit_transform(X)\n",
    "X_predict = scaler.transform(X_predict)\n",
    "print(\"minmax scaling complete\")\n",
    "\n",
    "#apply PCA if preferred\n",
    "r\"\"\"\n",
    "ValueError: Input X contains NaN.\n",
    "PCA does not accept missing values encoded as NaN natively. \n",
    "For supervised learning, you might want to consider \n",
    "sklearn.ensemble.HistGradientBoostingClassifier and Regressor \n",
    "which accept missing values encoded as NaNs natively. \n",
    "Alternatively, it is possible to preprocess the data, \n",
    "for instance by using an imputer transformer in a pipeline \n",
    "or drop samples with missing values. \n",
    "See https://scikit-learn.org/stable/modules/impute.html \n",
    "You can find a list of all estimators that handle NaN values \n",
    "at the following page: \n",
    "https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "pca = PCA(n_components=8)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test  = pca.transform(X_test)\n",
    "print(\"PCA application complete\")\n",
    "\"\"\"\n",
    "#NOW APPLY YOUR PREFERRED MODEL TYPE\n",
    "r\"\"\"\n",
    "ValueError: Input X contains NaN.\n",
    "MLPRegressor does not accept missing values encoded as NaN natively. \n",
    "For supervised learning, you might want to consider \n",
    "sklearn.ensemble.HistGradientBoostingClassifier and Regressor \n",
    "which accept missing values encoded as NaNs natively. \n",
    "Alternatively, it is possible to preprocess the data, \n",
    "for instance by using an imputer transformer in a pipeline \n",
    "or drop samples with missing values. \n",
    "See https://scikit-learn.org/stable/modules/impute.html \n",
    "You can find a list of all estimators that handle NaN values \n",
    "at the following page: \n",
    "https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "model = MLPRegressor(\n",
    "    hidden_layer_sizes=( 16, 8),\n",
    "    activation='logistic',\n",
    "    learning_rate='adaptive',\n",
    "    max_iter=400,\n",
    "    random_state=42\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=500,\n",
    "                              max_features='sqrt',\n",
    "                              max_depth = 400\n",
    ")\n",
    "\n",
    "\n",
    "# model = HistGradientBoostingRegressor(\n",
    "#     loss= \"absolute_error\",\n",
    "#     learning_rate= 0.1,\n",
    "#     max_iter= 100\n",
    "# )\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Train score:\", model.score(X_train, y_train))\n",
    "print(\"Test score:\", model.score(X_test, y_test))\n",
    "print('abs_loss',np.average(abs(model.predict(X_test)-y_test)))\n",
    "\n",
    "#FOR MAKING THE ACTUAL PREDICTIONS\n",
    "\n",
    "model.fit(X, y)\n",
    "y_predict = model.predict(X_predict)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"ID\": test_df[\"ID\"],\n",
    "    \"affinity_score\": y_predict\n",
    "})\n",
    "submission.to_csv(\"data/submission2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f3d6088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.21740757e+01  1.97405149e-01  1.36250000e+01  5.35652000e+02\n",
      "  0.00000000e+00 -4.92236956e-01  1.02500000e+00  1.64776751e+01\n",
      "  1.01260722e+01  2.14239096e+00 -2.15826678e+00  2.31433929e+00\n",
      " -2.24536150e+00  5.94712585e+00 -1.15752168e-01  3.65400968e+00\n",
      "  2.47479351e+09  0.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "  3.00000000e+00  2.00000000e+00  5.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  3.00000000e+00  1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  6.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  4.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "  1.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  4.00000000e+00  3.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  3.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  2.00000000e+00\n",
      "  0.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  1.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  1.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  1.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  1.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "  1.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  1.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "  1.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "  1.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "  1.00000000e+00  1.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "  1.00000000e+00  1.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  1.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  1.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  1.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  1.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "  1.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  1.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "  1.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  1.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "  1.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  1.00000000e+00  1.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  1.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "  1.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  1.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "  1.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  1.00000000e+00  1.00000000e+00  0.00000000e+00  8.41732283e+01\n",
      "  9.44881890e-02  1.64089001e-02  3.09576213e-06  3.64829396e-01\n",
      "  6.04789834e+00  5.15223097e+00  1.00944882e+01  1.43307087e+00\n",
      "  2.48031496e+00  6.03674541e-02  1.33747612e+02  2.61679790e+00\n",
      "  3.70078740e+00  8.23952756e+01  2.68582677e+00  2.94488189e+00\n",
      " -6.61942257e-02  9.75590551e-02  8.19422572e-02  2.59842520e-02\n",
      " -1.03149606e-02 -8.32020997e-03  1.38267717e-01  4.15485564e-02\n",
      "  4.21727749e+01  5.75916230e-02  1.22025536e-02 -1.51184198e-06\n",
      "  2.25130890e-01  6.06108197e+00  4.61518325e+00  9.09424084e+00\n",
      "  1.47643979e+00  2.52356021e+00  2.87958115e-02  1.26576047e+02\n",
      "  2.67801047e+00  3.72513089e+00  8.26853403e+01  2.89947644e+00\n",
      "  2.58376963e+00 -2.66570681e-01 -1.05340314e-01 -1.84188482e-01\n",
      "  1.44581152e-01 -6.46073298e-02  1.05052356e-01  1.87460733e-01\n",
      "  1.68167539e-01  7.63000000e+02]\n"
     ]
    }
   ],
   "source": [
    "print(X_predict[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
