{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528309e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/derekkentie/group-4.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf42cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle #to un-encode the dictionaries\n",
    "from pathlib import Path #used for looping through representation dictionaries\n",
    "from itertools import product #used for gridsearch on hyperparameters\n",
    "\n",
    "#data processing libraries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#model libraries\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73e74f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Modeltrainer:\n",
    "    \"\"\"\n",
    "    This program is made to perform a gigantic gridsearch, not ontly on model-\n",
    "    specific hyperparameters, but on all the different possible combinations\n",
    "    between: molecule and protein representation, representation-combination\n",
    "    method, scaling method, PCA options, ML model, and ML model specific\n",
    "    hyperparameters. The program will then store all its findings in an excel\n",
    "    with columns: train_score, test_score, combination.\n",
    "\n",
    "    IT IS STRONGLY ADVISED TO NOT RUN THIS PROGRAM ON YOUR \n",
    "    PERSONAL LAPTOP DUE TO EXTREME AMOUNT OF COMPUTER CALCULATION!\n",
    "    \n",
    "    Rather, make use of the available gpu's on Google Colab by importing the\n",
    "    complete folder, named 'group-4', on Colab and then running the current\n",
    "    program, named 'Model_trainer.py', while making use of the gpu's.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.dict_of_representation_combination_methods = {\n",
    "            \"feature concatenation\": self.feature_concatenation,\n",
    "            \"Late Fusion\": self.late_fusion,\n",
    "            \"Mid-level Fusion\": self.mid_level_fusion,\n",
    "            \"Cross-attention\": self.cross_attention,\n",
    "            \"End-to-end Multimodel Learning\": self.multimodel_learning\n",
    "            }\n",
    "        \n",
    "        self.dict_of_model_builders = {\n",
    "            \"Ridge\": self.build_ridge,\n",
    "            \"Lasso\": self.build_lasso,\n",
    "            \"Random Forest\": self.build_random_forest,\n",
    "            \"Gradient Boosting Regressor\": self.build_gbr,\n",
    "            \"Hist Gradient Boosting Regressor\": self.build_hgbr,\n",
    "            \"Support Vector Regression\": self.build_svr,\n",
    "            \"Multi-Layer Perceptron\": self.build_mlp,\n",
    "            \"K Nearest Neighbors\": self.build_knn,\n",
    "        }\n",
    "\n",
    "        self.dict_of_model_hyperparams = {\n",
    "            \"Ridge\": {\n",
    "                \"alpha\": [0.1, 1.0, 10.0, 100.0]\n",
    "            },\n",
    "            \"Lasso\": {\n",
    "                \"alpha\": [1e-4, 1e-3, 1e-2]\n",
    "            },\n",
    "            \"Random Forest\": {\n",
    "                \"n_estimators\": [200, 500],\n",
    "                \"max_depth\": [None, 20],\n",
    "                \"min_samples_split\": [2, 5]\n",
    "            },\n",
    "            \"Gradient Boosting Regressor\": {\n",
    "                \"n_estimators\": [200, 500],\n",
    "                \"learning_rate\": [0.05, 0.1],\n",
    "                \"max_depth\": [3, 5]\n",
    "            },\n",
    "            \"Hist Gradient Boosting Regressor\": {\n",
    "                \"max_iter\": [200, 500],\n",
    "                \"learning_rate\": [0.05, 0.1],\n",
    "                \"max_depth\": [6, 8, 10],\n",
    "                \"max_bins\": [128, 255]\n",
    "            },\n",
    "            \"Support Vector Regression\": {\n",
    "                \"kernel\": [\"rbf\"],\n",
    "                \"C\": [1.0, 10.0, 100.0],\n",
    "                \"gamma\": [\"scale\"]\n",
    "            },\n",
    "            \"Multi-Layer Perceptron\": {\n",
    "                \"hidden_layer_sizes\": [(256, 128), (512, 256)],\n",
    "                \"learning_rate_init\": [1e-3, 1e-4],\n",
    "                \"alpha\": [1e-4],\n",
    "                \"max_iter\": [500]\n",
    "            },\n",
    "            \"K Nearest Neighbors\": {\n",
    "                \"n_neighbors\": [5, 10, 20],\n",
    "                \"weights\": [\"distance\"]\n",
    "            }\n",
    "        }\n",
    "\n",
    "        self.dict_of_data_processors = {\n",
    "            \"scaler\": [None, self.standard_scaling, self.minmax_scaling],\n",
    "            \"pca\": [None, 1, 2, 3, 4]\n",
    "        }\n",
    "    \n",
    "    def data_loader(self, data_location):\n",
    "        data = pd.read_csv(rf\"{data_location}\")\n",
    "        return data\n",
    " \n",
    "    def representation_gridsearch(self, folder_location):\n",
    "        \"\"\"\n",
    "        Returns a list of filepaths from that representations that are\n",
    "        included in the representation-folder.\n",
    "        \"\"\"\n",
    "        representation_folder = Path(folder_location)\n",
    "        return list(representation_folder.iterdir())\n",
    "\n",
    "    def pickle_converter(self, pickle_file_path):\n",
    "        \"\"\"\n",
    "        Returns the unencoded/unpickled document of pickle_file_path.\n",
    "        \"\"\"\n",
    "        with pickle_file_path.open(\"rb\") as pickle_file_handle:\n",
    "            return pickle.load(pickle_file_handle)\n",
    "\n",
    "    def dicts_collector(self, folder_location):\n",
    "        \"\"\"\n",
    "        Returns a dictionary of dictionaries from both the molecule and\n",
    "        protein representations, with key equal to file_path_name and item\n",
    "        equal to the respective representation dicitionary which is \n",
    "        retreived using the pickle_converter function.\n",
    "        \"\"\"\n",
    "        pickle_file_paths = self.representation_gridsearch(rf\"{folder_location}\")\n",
    "        dict_of_rep_dicts = {}\n",
    "        for pickle_file_path in pickle_file_paths:\n",
    "            file_path_name = pickle_file_path.stem #collects only the file name from the Path object\n",
    "            dict_of_rep_dicts[file_path_name] = self.pickle_converter(pickle_file_path)\n",
    "        if len(pickle_file_paths) == len(dict_of_rep_dicts):\n",
    "            return dict_of_rep_dicts\n",
    "        else:\n",
    "            raise AttributeError(\n",
    "                \"something went wrong with extracting the dictionaries in the dict_collector function\"\n",
    "            )\n",
    "    \n",
    "    def hyperparam_generator(self, model_name):\n",
    "        grid = self.dict_of_model_hyperparams[model_name]\n",
    "        keys = grid.keys()\n",
    "        values = grid.values()\n",
    "\n",
    "        for combination in product(*values):\n",
    "            yield dict(zip(keys, combination))   \n",
    "\n",
    "    def model_builder(self, model_name, hyperparams):\n",
    "        \"\"\"\n",
    "        Returns the application of a combinations of hyperparameters\n",
    "        to a model.\n",
    "        \"\"\"\n",
    "        model_builder = self.dict_of_model_builders[model_name]\n",
    "        return model_builder(hyperparams)\n",
    " \n",
    "    def experiment_maker(self, mol_folder_location, protein_folder_location, one_or_all_combination_method = \"feature concatenation\"):\n",
    "        \"\"\"\n",
    "        Returns every possible combination between de representations,\n",
    "        representation-combination methods, models and hyperparameters,\n",
    "        in a list of dictionaries.\n",
    "        \"\"\"\n",
    "        experiments = []\n",
    "\n",
    "        #selection for one or all combination methods, feature concatenation is now used \n",
    "        if one_or_all_combination_method == \"all\":\n",
    "            comb_method = self.dict_of_representation_combination_methods.items()\n",
    "        else:\n",
    "            comb_method = [(one_or_all_combination_method, self.dict_of_representation_combination_methods[one_or_all_combination_method])]\n",
    "\n",
    "        # 1. Load representations\n",
    "        mol_rep_dict_of_dicts = self.dicts_collector(\n",
    "            mol_folder_location\n",
    "        )\n",
    "        protein_rep_dict_of_dicts = self.dicts_collector(\n",
    "            protein_folder_location\n",
    "        )\n",
    "\n",
    "        # 2. Loop over molecule & protein representations\n",
    "        for (mol_rep_name, mol_rep_dict), (prot_rep_name, prot_rep_dict) in product(\n",
    "            mol_rep_dict_of_dicts.items(),\n",
    "            protein_rep_dict_of_dicts.items()\n",
    "        ):\n",
    "            # 3. Loop over representation-combination methods\n",
    "            for comb_name, comb_function in comb_method:\n",
    "\n",
    "                # 4. Loop over models\n",
    "                for model_name in self.dict_of_model_builders.keys():\n",
    "\n",
    "                    # 5. Loop over hyperparameter combinations (generator!)\n",
    "                    for hyperparams in self.hyperparam_generator(model_name):\n",
    "                        \n",
    "                        for scale_type in self.dict_of_data_processors[\"scaler\"]:\n",
    "\n",
    "                            for pca in self.dict_of_data_processors[\"pca\"]:\n",
    "\n",
    "                                # 6. appending every possible combination as a dictionary in the list experiments\n",
    "                                experiments.append({\n",
    "                                    \"molecule_representation_name\": mol_rep_name,\n",
    "                                    \"molecule_representation_dict\": mol_rep_dict,\n",
    "\n",
    "                                    \"protein_representation_name\": prot_rep_name,\n",
    "                                    \"protein_representation_dict\": prot_rep_dict,\n",
    "\n",
    "                                    \"combination_method_name\": comb_name,\n",
    "                                    \"combination_method_function\": comb_function,\n",
    "\n",
    "                                    \"model_name\": model_name,\n",
    "                                    \"hyperparameters\": hyperparams,\n",
    "\n",
    "                                    \"scale_type\": scale_type,\n",
    "                                    \"pca\": pca\n",
    "                                })\n",
    "\n",
    "        return experiments\n",
    "    \n",
    "    def init_results_csv(self, output_csv_path):\n",
    "        output_csv_path = Path(output_csv_path)\n",
    "\n",
    "        if not output_csv_path.exists():\n",
    "            print(f\"creating {output_csv_path} as new path\")\n",
    "            df = pd.DataFrame(columns=[\n",
    "                \"mol_representation\",\n",
    "                \"combination_method\",\n",
    "                \"model\",\n",
    "                \"hyperparams\",\n",
    "                \"seed\",\n",
    "                \"train_score\",\n",
    "                \"train_accuracy\",\n",
    "                \"test_score\",\n",
    "                \"test_accuracy\",\n",
    "                \"error\"\n",
    "            ])\n",
    "            df.to_csv(output_csv_path, index=False)\n",
    "        else:\n",
    "            print(f\"{output_csv_path} already exists\")\n",
    "    \n",
    "    def experiment_tester(self, output_csv_path, data, mol_folder_location, protein_folder_location, debug_fraction_selector = 1):\n",
    "        experiments = self.experiment_maker(mol_folder_location, protein_folder_location)\n",
    "        self.init_results_csv(output_csv_path)\n",
    "        \n",
    "        for i, experiment in enumerate(experiments, start=1):\n",
    "\n",
    "            row = {\n",
    "                \"mol_representation\": experiment[\"molecule_representation_name\"],\n",
    "                \"protein_representation\": experiment[\"protein_representation_name\"],\n",
    "                \"combination_method\": experiment[\"combination_method_name\"],\n",
    "                \"model\": experiment[\"model_name\"],\n",
    "                \"hyperparams\": str(experiment[\"hyperparameters\"]),\n",
    "                \"seed\": experiment.get(\"seed\", None),\n",
    "                \"train_score\": \"None\",\n",
    "                \"train_accuracy\": \"None\",\n",
    "                \"test_score\": \"None\",\n",
    "                \"test_accuracy\": \"None\",\n",
    "                \"error\": \"None\"\n",
    "            }\n",
    "            if i/debug_fraction_selector == int(i/debug_fraction_selector):\n",
    "                try:\n",
    "                    model_type = self.dict_of_model_builders[experiment[\"model_name\"]]\n",
    "                    model = model_type(experiment[\"hyperparameters\"])\n",
    "                    \n",
    "                    combination_method = self.dict_of_representation_combination_methods[experiment[\"combination_method_name\"]]\n",
    "                    X, y = combination_method(experiment[\"molecule_representation_dict\"], experiment[\"protein_representation_dict\"], data)\n",
    "\n",
    "                    X_train, y_train, X_test, y_test = self.train_test_split(X, y)\n",
    "\n",
    "                    if self.dict_of_data_processors[\"scaler\"] != None:\n",
    "                        X_train, X_test = self.dict_of_data_processors[\"scaler\"](X_train, X_test)\n",
    "\n",
    "                    if self.dict_of_data_processors[\"pca\"] != None:\n",
    "                        X_train, X_test = self.pca(X_train, X_test)\n",
    "\n",
    "                    model.fit(X_train, y_train)\n",
    "\n",
    "                    y_train_pred = model.predict(X_train)\n",
    "                    y_test_pred = model.predict(X_test)\n",
    "\n",
    "                    row[\"train_score\"] = model.score(X_train, y_train)\n",
    "                    row[\"train_accuracy\"] = self.accuracy(y_train, y_train_pred)\n",
    "\n",
    "                    row[\"test_score\"] = model.score(X_test, y_test)\n",
    "                    row[\"test_accuracy\"] = self.accuracy(y_test, y_test_pred)\n",
    "\n",
    "\n",
    "\n",
    "                except Exception as e:\n",
    "                    row[\"error\"] = str(e)\n",
    "\n",
    "                if i % 100 == 0:\n",
    "                    print(f\"{i}/{len(experiments)} experiments calculated\")\n",
    "            \n",
    "                pd.DataFrame([row]).to_csv(\n",
    "                output_csv_path,\n",
    "                mode=\"a\",\n",
    "                header=False,\n",
    "                index=False\n",
    "                )\n",
    "\n",
    "        \n",
    "###############    Model builders   ###############\n",
    "    def build_ridge(self, hyperparams):\n",
    "        return Ridge(**hyperparams)\n",
    "    \n",
    "    def build_lasso(self, hyperparams):\n",
    "        return Lasso(**hyperparams)    \n",
    "    \n",
    "    def build_random_forest(self, hyperparams):\n",
    "        return RandomForestRegressor(**hyperparams)\n",
    "    \n",
    "    def build_gbr(self, hyperparams):\n",
    "        return GradientBoostingRegressor(**hyperparams)\n",
    "    \n",
    "    def build_hgbr(self, hyperparams):\n",
    "        return HistGradientBoostingRegressor(**hyperparams)\n",
    "    \n",
    "    def build_svr(self, hyperparams):\n",
    "        return SVR(**hyperparams)\n",
    "    \n",
    "    def build_mlp(self, hyperparams):\n",
    "        return MLPRegressor(**hyperparams)\n",
    "    \n",
    "    def build_knn(self, hyperparams):\n",
    "        return KNeighborsRegressor(**hyperparams)\n",
    "###################################################\n",
    "\n",
    "\n",
    "############### Combination methods ###############\n",
    "       \n",
    "    def feature_concatenation(self, molecule_feature_dict, protein_feature_dict, data):\n",
    "        \"\"\"\n",
    "        Returns a numpy array of the concetenated features form the molecule\n",
    "        and protein feature dictionaries. \n",
    "\n",
    "        This function checks if every item from the datafile is exists in both\n",
    "        feature dictionaries, and will otherwise raise an error.\n",
    "\n",
    "        If the datafile contains affinity scores (which means the data file\n",
    "        is the train.csv file) it will collect those in the numpy array y and \n",
    "        also return it.\n",
    "        \"\"\"\n",
    "        X = []\n",
    "        y = []\n",
    "\n",
    "        #feature concatenation to combining each ligand-protein pair\n",
    "        for _, row in data.iterrows():\n",
    "            smiles = row[\"molecule_SMILES\"]\n",
    "            protein = row[\"UniProt_ID\"]\n",
    "            affinity_score = row[\"affinity_score\"]\n",
    "\n",
    "            #quick check if all elements are available\n",
    "            if smiles not in molecule_feature_dict: \n",
    "                raise FileNotFoundError(\n",
    "                    f\"The following SMILES exists in the trainingset but not in the molecule-features dictionary: {smiles}\"\n",
    "                )\n",
    "            if protein not in protein_feature_dict: \n",
    "                raise FileNotFoundError(\n",
    "                    f\"The following Uniprot_ID exists in the trainingset but not in the protein-features dictionary: {protein}\"\n",
    "                )\n",
    "        \n",
    "\n",
    "            #feature concatenation\n",
    "\n",
    "            #making sure that both representations are in list form\n",
    "            if isinstance(molecule_feature_dict[smiles], np.ndarray):\n",
    "                molecule_feature_dict[smiles] = molecule_feature_dict[smiles].tolist()\n",
    "            if isinstance(protein_feature_dict[protein], np.ndarray):\n",
    "                protein_feature_dict[protein] = protein_feature_dict[protein].tolist()\n",
    "\n",
    "            combined = molecule_feature_dict[smiles] + protein_feature_dict[protein]\n",
    "\n",
    "            #data seperation\n",
    "            X.append(combined)\n",
    "            y.append(affinity_score)\n",
    "        X = np.array(X, dtype=float)\n",
    "        y = np.array(y, dtype=float)\n",
    "        return X, y\n",
    "\n",
    "    def late_fusion(self):\n",
    "        pass\n",
    "    \n",
    "    def mid_level_fusion(self):\n",
    "        pass\n",
    "\n",
    "    def cross_attention(self):\n",
    "        pass\n",
    "\n",
    "    def multimodel_learning(self):\n",
    "        pass\n",
    "###################################################\n",
    "\n",
    "\n",
    "###############   Data processing   ###############\n",
    "    def standard_scaling(self, X_train, X_test):\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        return X_train, X_test\n",
    "\n",
    "    def minmax_scaling(self, X_train, X_test):\n",
    "        scaler = MinMaxScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        return X_train, X_test\n",
    "\n",
    "    def pca(self, X_train, X_test):\n",
    "        pca = PCA(self.dict_of_data_processors[\"pca\"])\n",
    "        X_train = pca.fit_transform(X_train)\n",
    "        X_test  = pca.transform(X_test)\n",
    "        return X_train, X_test\n",
    "\n",
    "    def train_test_split(self, X, y):\n",
    "        return train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "###################################################\n",
    "\n",
    "    def accuracy(self, y_true, y_pred):\n",
    "        y_true = np.array(y_true)\n",
    "        y_pred = np.array(y_pred)\n",
    "        return np.mean(y_true == y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bc578f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch = Modeltrainer()\n",
    "data = gridsearch.data_loader(r\"/content/group-4/data/train.csv\")\n",
    "gridsearch.experiment_tester(r\"/content/group-4/docs/model_gridsearch.csv\", \n",
    "                             data, \n",
    "                             mol_folder_location = r\"/content/group-4/docs/mol representatie picklebestanden\",\n",
    "                             protein_folder_location = r\"/content/group-4/docs/Sep's picklebestanden/protein dicts to use in gridsearch\",\n",
    "                             debug_fraction_selector= 100)\n",
    "\n",
    "gridsearch_csv = pd.read_csv(r\"/content/group-4/docs/model_gridsearch.csv\")\n",
    "print(gridsearch_csv[\"error\"].unique())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
